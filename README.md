# Вступление

В детстве я часто скачивал вебсайты, так как был модем и интернет в 33 килобита с почасовой оплатой. Читать документацию по программированию в онлайне - непозволительная роскошь. Потому как только в поисковике находишь годный сайт по нужной теме - скачиваешь как минимум 1 страницу, а то и несколько страниц в глубь него. По всей видимости, это нанесло мне глубокую психологическую травму. Сейчас, когда я читаю книгу "Программирование на языке GO", дохожу до примеров по написанию веб-скраперов, то у меня перед глазами возникают въетнамские флешбеки с Диско-Качалками, Телепортами и ВебЗипами, равно как и детская мечта "написать такое же, только круче и без баннеров".

Впрочем, сегодня я бы тоже не отказался от такой утилиты, которая бы позволила корректно выкачивать вебсайты и иметь полностью оффлайновую документацию, на манер MSDN, который поставлялся на нескольких CD-дисках. Утилита должна уметь:
* Корректно выкачивать все что видит и сохранять все данные, включая заголовки
* Понимать, что ходит по кругу и выкачивает однотипные страницы, вырезать мусор из url, такой как phpsessid
* Корректно отображать выкачанное, в идеале конвертировать ссылки
* Делать из этого PDF/CHM/EPUB, обрезать рекламу и лишние части шаблона
* Делать какое-то оглавление и возможно поиск по скачанному

Самое юзабельное, что есть на данный момент под рукой - это `wget -p`. К сожалению, `wget -p` выкачивает далеко не все зависимости страницы (как повезет), если на сайте есть менюшка на JS, то он ее не увидит, не умеет сливать в единый адрес http://example.com/simedir и http://example.com/simedir/, причем первый вариант не переименовывает в html, из-за чего браузер не всегда знает как это отображать. В общем, если очень надо, то можно рискнуть, но надежды мало, из-за чего желания им пользоваться нет.

Готовые паки документации, такие как Zeal или Velocity работают через жопу и можно сказать, что их скорее нет, чем они есть.

# Что делать?

Написание своего скачивателя интернета лучше всего начать со скачивания всего интернета, а потом разбираться, что с этим делать.

## Пишем свое расширение, эмулирующее пользователя на странице
Задача простая, кликнуть на страничке, поскроллить ее, найти формочку, написать в ней test. Таким образом запустить внутренние скрипты, которые бы полезли в инет и что-то докачали за собой. Можно подождать onload и несколько раз поскроллить страницу вниз, на случай динамической подгрузки лент и прочих ассетов. Можно сделать скриншот экрана (страницы).

## Пишем еще одно расширение, логгирующее сетевые запросы
Пока webrequest окончательно из хрома не выпилили, то пишем еще один логгер. Просто записываем все, до чего можем дотянуться и отсылаем на свой сервер. К великому сожалению, до тела запроса достучаться нельзя, а пересобирать хром/фокс я явно не осилю, так что придется выкачивать эти ресурсы своим сервером еще раз, но полностью эмулируя браузерные запросы, включая реферрер и юзер-агент. При полной загрузке, даем управление предыдущему расширению. Можно сделать скриншот экрана (страницы).

## Простенький кеш
Имеет простую вебмордочку, куда расширение сливает запросы и просто реплеит их, т.е. отправляет как есть. Ответы аккуратно сохраняет в локальной субд, сохраняя каждый заголовок, каждый байт тела, таймштампы и все, что только можно сохранить. Если запрос закончился редиректом - сохраняем редирект, если пришла 500-я ошибка - сохраняем 500-ю ошибку именно как 500, а не 200. Если запрос пришел еще раз, но уже с новой кукой - еще раз лезет на сервер и еще раз выкачивает страницы, результат сохраняет рядом, не перезаписывая прошлый, подсчитывает хеши, указывает что это дубль или новая страница. Публикует все сохраненные страницы в своей вебморде, чтобы можно было найти все адреса, куда ломился браузер. В идеале сделать некий grep, чтобы сразу видеть, откуда какой url пришел или из каких частей был построен. Может представляться простым http-прокси, который возвращает ресурс по URL. Так как мы кешируем еще и POST-запросы, то неплохо бы отвечать и на них. Само собой, ответы должны быть на 100% идентичными, включая всякие Set-Cookie и тому подобный кал. Кеш должен быть простым, тупым и неубиваемым, с нерушимой базой данных.

## Делаем базу самых лучших сайтов
Где-то в 2012-м я уже скачивал весь интернет по запросу "скачать бесплатно дискотека авария mp3", после чего у меня в виртуалке отвалилась сеть и перестал работать NAT. Само собой, это было сделано через парсинг гугла/бинг/дакдакго по запросу уровня выше, построен списочек "самых лучших сайтов", а дальше был сделан отдельный пользователь с браузером, где было разрешено все, включая автозапуск флеша. Где оно бродило я не знаю, но жаль, что результатов не сохранилось. Вот, видимо пришло время повторить опыт.

## Скачиваем весь интернет
Берем последнюю версию хрома, фокса, ставим минимальные настройки безопасности. В идеале бы еще поставить флеш, но это наверное надо искать старые версии. Зачетно было бы повторить это все с IE старых версий, но увы, уже не актуально, а жаль.

И установив описанные выше плагины идем скачивать весь интернет, по списку "самых лучших сайтов". Все должно осесть в нашем кеше и мы должны получить скриншоты вебсайтов.

## Пытаемся смотреть сайты из кеша
В общем-то аналогично, только выставляем настройки прокси в наших браузерах и идем серфить эти же сайты, но по http. Делаем скриншоты, попиксельно сравниваем с записанными ранее. Очевидно, что многие сайты будут ломиться на https, что их поломает. В выхлопе кеша пытаемся отламывать https и заменять его на http, и так для каждого сайта, чьи скриншоты отличаются.

## Пытаемся парсить HTML/JS/CSS
Само собой, это невозможно без полной реализации браузера. Но у нас уже есть список запросов, который должен получиться, так что играемся с регулярками и подгоняем результат под уже известный. Делаем как позитивное, так и негативное тестирование - не должно быть таких запросов, которых не было в оригинале. Именно тут будет основное отличие от `wget -p` - будет сразу видно, каких woff-шрифтов оно не нашло и почему. Если в кеше эти шрифты есть, а мы не смогли найти на них ссылку - надо написать еще ведро регулярных выражений.

## Конвертирование ссылок
Распарсить ссылки - половина задачи. Еще надо их корректно заменить на новые адреса.

??? Тут надо дописать как не ходить кругами, чистить ссылки и тому подобное.

## Экспорт HTML/CSS/PNG
До сих пор наши данные лежали как блобы в базе данных, открывать их можно только через нашу проксю, что приемлемо, но несколько неудобно. Хотелось бы иметь именно html-файлы на диске и лежащие рядом картинки и прочие ресурсы. Сделать прямую трансляцию ресурсов в имена файлов - нельзя, на пороге не 2000-й год, да и утилиты в 2000-м году это уже умели делать. Задачи:
* Унифицировать http://example.com и http://www.example.com
* Унифицировать http://example.com/dir и http://example.com/dir/ до example.com/dir/index.html
* Унифицировать http://example.com/?param1=1&param2=2 и http://example.com/?param2=2&param1=1 и выплюнуть обе ссылки как один файл (естественно, если контент по ним совпадает)
* Упростить http://example.com/?param2=2&param1=1 до example.com/index-params-1-2.html
* Опционально упразднить имена файлов и ассеты экспортировать как a.png, b.png вместо longfilename.png и some/path/otherfile.png
* Опционально обрезаем шаблон в HTML. Можно как вырезать какие-то части и заменять на свои, так и просто заменять весь шаблон сайта, вырезая логотипы/хеадеры/футеры

## Экспорт как CHM
Аналогично варианту выше с обфускацией имен файлов, так как внутри архива они не нужны. В довесок получаем бесплатный поиск, оглавление, не храним ворох мелких файлов на файловой системе.

## Экспорт как PDF/Печатная версия
Склеиваем все странички, вместо URL проставляем якоря, ссылки конвертим на эти самые якоря.

# Сделать ко всему этому красивую морду на дельфи

Чтобы как в старые добрые времена. Правда я никогда не программил на дельфи, у меня был Ц++ Буилдер.

# Зачем?

Делать все это я конечно же не буду. Или буду. Да и вообще, половину всего этого я уже сделал в разные годы. Но основная задача этого текста - сдампить его из головы. ДАЙТЕ МНЕ ПРОСТО ДОЧИТАТЬ УЧЕБНИК ПО ГО.
